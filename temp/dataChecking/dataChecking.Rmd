---
title: "Checking Alberto's Dataset"
author: "Tati Micheletti"
date: "3/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Dataset Checking

This `.rmd` file was created to make a quality check on Alberto's final dataset (Final_points_BEAD.csv) file for the paper: **The rise of industrial development, the fall of boreal songbirds.**. It checks the dataset for duplicates and also corrects the number of samples per question (Appendix 1).

### Data uploading

The first task is to upload the data and load the required libraries:
```{r dataUploading}

# load libraries
library(data.table)
library(reproducible)
dataset <- "Final_points_BEAD.csv"
DT <- suppressWarnings(fread(file.path(getwd(),"modules/glmerBirdModels/data", dataset)))
```

### Check for duplicates
Then we check the variables the data table has, create a vector of which have the possible duplicated data and create a table for the checking of duplicates
```{r checkDuplicates}
variables <- names(DT)
variablesIgnore <- c("PKEY","SS")
source(file = file.path(getwd(),"checkDuplicates.R"))
duplicates <- checkDuplicates(dataTable = DT, ignoreCols = variablesIgnore)
knitr::kable(DT[duplicates$ROWS,])
```

This shows us that we have `r length(duplicates$ROWS)/2` duplicates that have different SS and PKEY, but everything else (including dates, times, coordinates and abundances) identical.

## Data Subsetting

Now we create variables to subset our data:
```{r createVariables}
typeDisturbance = c("Transitional", "Permanent", "Both", "Undisturbed")
disturbanceDimension = c("local", "neighborhood")
```

And we use a customized function to subset the dataset:
```{r dataSubsetting}
source(file = file.path(getwd(),"modules/glmerBirdModels/R/dataUploading.R"))
subsettedDT <- suppressMessages(dataUploading(data = dataset, disturbanceDimension = disturbanceDimension, typeDisturbance = typeDisturbance))
```

Now we can create a table with each subset sample sizes:
```{r sampleSizeTable}
source(file = file.path(getwd(),"subsetTable.R"))
subsettedTable <- subsetTable(dataset = subsettedDT)
knitr::kable(subsettedTable)
```

## SOLUTION

I propose we randomly delete duplicated values from the dataset and re-run the models using `SpaDES`.

## Cleaning out the duplicates

We have to double check the duplicates to see how they are organized. It is easier to inspect them in excel:
``` {r doubleCheckingDuplicates}
fwrite(DT[duplicates$ROWS,], "Final_points_BEAD_duplicates.csv") #Then we open the excel file
```

We saw that the variables are organized in pairs, which means that every other value from the `duplicates$ROWS` is a duplicate. So we delete these:
``` {r deleteDups}
rowsToRemove <- duplicates$ROWS[c(TRUE, FALSE)]
newDT <- DT[-rowsToRemove,]
```

Now we run again our duplicates test to ensure we don't have any duplicated left:
```{r checkDuplicates2}
duplicates2 <- checkDuplicates(dataTable = newDT, ignoreCols = variablesIgnore)
DT[duplicates2$ROWS,]
```

Now that we are sure that all duplicates are gone, we save the new .csv file to use for the models:
``` {r writeNewTable}
fwrite(newDT, "modules/glmerBirdModels/data/Final_points_BEAD_final.csv")
```

Now we use this corrected `Final_points_BEAD_final.csv` dataset instead of the original `Final_points_BEAD.csv`.  


